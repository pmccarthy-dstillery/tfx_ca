{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib.resources as pkg_resources\n",
    "import kfp\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp_ca import config\n",
    "from kfp_ca import sql as sql_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_qry = pkg_resources.read_text(sql_dir,'big_query_extract_dataset.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeded_bq_query(seed, qry, bucket, name_str, ts) -> NamedTuple('DataFileLocation',[('bucket',str),('filename',str)]):\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    from google.cloud import bigquery, storage\n",
    "    bqclient = bigquery.Client()\n",
    "    \n",
    "    spaced_seed = seed.replace('|',' | ')\n",
    "    quoted_seed = '\" '+seed+' \"'\n",
    "    \n",
    "    df = bqclient.query(qry.replace(\"SEEDS\",quoted_seed)).to_dataframe()\n",
    "    df.drop('rnk',axis=1).to_feather('/tmp/df.feather')\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket)\n",
    "    \n",
    "    filename = f\"{name_str}.{ts}.feather\"\n",
    "    \n",
    "    blob = bucket.blob(filename)\n",
    "    blob.upload_from_filename('/tmp/df.feather')\n",
    "    \n",
    "    return blob.bucket.name, blob.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_train(input_bucket: str, \n",
    "                       input_file:str, \n",
    "                       output_bucket:str, \n",
    "                       ts:str, \n",
    "                       model_label:str) -> NamedTuple('ModelOutput',[('bucket',str),\n",
    "                                                                     ('model_label',str),\n",
    "                                                                     ('eval_data',str)]):\n",
    "    \n",
    "    import joblib\n",
    "    import pandas as pd    \n",
    "    from google.cloud import storage\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import traceback\n",
    "\n",
    "    def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "        \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "        # source_blob_name = \"storage-object-name\"\n",
    "        # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Construct a client side representation of a blob.\n",
    "        # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "        # any content from Google Cloud Storage. As we don't need additional data,\n",
    "        # using `Bucket.blob` is preferred here.\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "\n",
    "        \n",
    "    def single_uploader(local_file, bucket):\n",
    "\n",
    "        stripped_file_name = local_file.split('/')[-1]\n",
    "\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(bucket)\n",
    "        blob = bucket.blob(stripped_file_name)\n",
    "        blob.upload_from_filename(local_file)\n",
    "\n",
    "    download_blob(input_bucket,input_file,f'/tmp/df-{ts}.feather')\n",
    "    \n",
    "    df = pd.read_feather(f'/tmp/df-{ts}.feather')\n",
    "        \n",
    "    y = df['label'].values\n",
    "    X = df['content'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "    \n",
    "    eval_label = f'evaldata-{model_label}-{ts}.pkl'\n",
    "    with open(\"/tmp/\"+eval_label,'wb') as f:\n",
    "        joblib.dump((X_test,y_test),f)\n",
    "        print(f\"Xtest shape: {X_test.shape}\")\n",
    "    \n",
    "    single_uploader(\"/tmp/\"+eval_label, output_bucket)\n",
    "\n",
    "    cls = SGDClassifier(loss='log',\n",
    "        penalty='elasticnet',\n",
    "        learning_rate='adaptive',\n",
    "        eta0=2,\n",
    "        verbose=1,\n",
    "        tol=1e-2,\n",
    "        max_iter=10)\n",
    "\n",
    "    count_vectorizer = CountVectorizer()\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', count_vectorizer),\n",
    "        ('cls', cls)\n",
    "        ])\n",
    "\n",
    "    grid = {'cls__alpha':[0.01,0.5,0.99,2,5], \n",
    "            'cls__l1_ratio':[0.01,0.5,0.99]}\n",
    "\n",
    "#     grid_search_cv = GridSearchCV(pipeline, param_grid=grid, scoring='roc_auc')\n",
    "\n",
    "\n",
    "    model = pipeline.fit(X=X_train.ravel(), y=y_train)\n",
    "\n",
    "    model_label = f'{model_label}-{ts}.joblib'\n",
    "    \n",
    "\n",
    "    \n",
    "    joblib.dump(model,f'/tmp/{model_label}')\n",
    "\n",
    "    single_uploader(f'/tmp/{model_label}', output_bucket)\n",
    "        \n",
    "    return output_bucket, model_label, eval_label \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_component(bucket, eval_data, model_file) -> NamedTuple('Outputs', [\n",
    "  ('mlpipeline_metrics', 'Metrics'),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import joblib\n",
    "    import json\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import pandas as pd    \n",
    "    \n",
    "    def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "        \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "        \n",
    "        \n",
    "    def top_and_bottom(model):\n",
    "        \n",
    "\n",
    "        # {id:'contentid'}\n",
    "        inverse_dictionary = {model[0].vocabulary_[k]:k for k in model[0].vocabulary_.keys()}\n",
    "\n",
    "        dictionary_df = (\n",
    "            pd.DataFrame(inverse_dictionary.items())\n",
    "            .rename(columns={0:'id', 1:'content_id'})\n",
    "        ).set_index('id')\n",
    "\n",
    "        coef_df = pd.DataFrame({\n",
    "            'coef':model[1].coef_[0],\n",
    "            'id':range(len(model[1].coef_[0]))}).set_index('id')\n",
    "\n",
    "        content_df = dictionary_df.join(coef_df).set_index('content_id')\n",
    "\n",
    "        all_df = (\n",
    "            content_df.join(macro_content_df)\n",
    "            .sort_values(\"coef\", ascending=False))\n",
    "\n",
    "        return all_df\n",
    "        \n",
    "                \n",
    "    download_blob(bucket, eval_data, '/tmp/eval_data.joblib')\n",
    "    \n",
    "    X, y = joblib.load('/tmp/eval_data.joblib')\n",
    "\n",
    "    download_blob(bucket, model_file, '/tmp/model_file.joblib')\n",
    "\n",
    "    model = joblib.load('/tmp/model_file.joblib')\n",
    "\n",
    "    download_blob(bucket, 'macro_content.csv', '/tmp/macro_content.csv')\n",
    "    macro_content_df = pd.read_csv('/tmp/macro_content.csv').set_index('content_id')\n",
    "    print(top_and_bottom(model))\n",
    "        \n",
    "    preds = model.predict_proba(X)[:,1]\n",
    "    accuracy_score = model.score(X,y)\n",
    "    roc_auc_score = roc_auc_score(y_true=y,y_score=preds)\n",
    "    print(f\"AUC Score: {roc_auc_score}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'metrics': [{\n",
    "          'name': 'Accuracy', # The name of the metric. Visualized as the column name in the runs table.\n",
    "          'numberValue':  accuracy_score, # The value of the metric. Must be a numeric value.\n",
    "          'format': \"PERCENTAGE\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "        },\n",
    "        {\n",
    "            'name':'AUC',\n",
    "            'numberValue': roc_auc_score,\n",
    "            'format':'PERCENTAGE',\n",
    "        }]\n",
    "      }\n",
    "    \n",
    "    return [json.dumps(metrics)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_op = kfp.components.func_to_container_op(func=seeded_bq_query,\n",
    "                                                 packages_to_install=['google-cloud-storage',\n",
    "                                                                      'google-cloud-bigquery-storage',\n",
    "                                                                      'pyarrow',\n",
    "                                                                      'pandas',\n",
    "                                                                      'google-cloud-bigquery'],\n",
    "                                                 output_component_file='seeded_bq_op.yaml')\n",
    "\n",
    "train_op = kfp.components.func_to_container_op(func=download_and_train,\n",
    "                                                 packages_to_install=['google-cloud-storage',\n",
    "                                                                      'pandas',\n",
    "                                                                      'pyarrow',\n",
    "                                                                      'scikit-learn'],\n",
    "                                                 output_component_file='download_op.yaml')\n",
    "\n",
    "metrics_op = kfp.components.func_to_container_op(func=metrics_component,\n",
    "                                                packages_to_install=['google-cloud-storage',\n",
    "                                                                     'scikit-learn',\n",
    "                                                                     'pandas'],\n",
    "                                                output_component_file='metrics_op.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host='3fad3dc513a320a4-dot-us-central2.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='Scikit_KFP',\n",
    "    description=''\n",
    ")\n",
    "def my_pipeline_func(\n",
    "    qry,\n",
    "    seed,    \n",
    "    bq_output_path,\n",
    "    ts,\n",
    "    model_label\n",
    "):\n",
    "\n",
    "    bq_op_output = bq_op(seed, qry, bq_output_path, name_str=\"bq_data\",ts=ts)\n",
    "    \n",
    "    train_op_output = train_op(input_bucket=bq_op_output.outputs['bucket'], \n",
    "                 input_file=bq_op_output.outputs['filename'],\n",
    "                 output_bucket=bq_output_path,\n",
    "                 ts=ts,\n",
    "                 model_label=model_label)\n",
    "    \n",
    "    metrics_output = metrics_op(bucket=train_op_output.outputs['bucket'],\n",
    "                                eval_data=train_op_output.outputs['eval_data'],\n",
    "                                model_file=train_op_output.outputs['model_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://3fad3dc513a320a4-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/571d2171-a7e3-4214-b86a-b49e04d89483\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://3fad3dc513a320a4-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/fc206aa6-a063-4ee6-bf04-0e82adb1568d\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_label = 'OL7454 - Music Fans'\n",
    "model_seed = 'ff3r|ckxv'\n",
    "\n",
    "pipeline_args = {\n",
    "    'qry':src_qry,\n",
    "    'seed':model_seed,\n",
    "    'bq_output_path':'kfpca_bq',\n",
    "    'ts':datetime.datetime.now().isoformat(),\n",
    "    'model_label':model_label\n",
    "}\n",
    "\n",
    "my_run = client.create_run_from_pipeline_func(pipeline_func=my_pipeline_func,\n",
    "                                     arguments=pipeline_args,\n",
    "                                     run_name=f\"{model_label}-{datetime.datetime.now().isoformat()}\",\n",
    "                                     experiment_name=\"KFP Demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://3fad3dc513a320a4-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/571d2171-a7e3-4214-b86a-b49e04d89483\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://3fad3dc513a320a4-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/d9d5c6b5-5d31-47b9-b8ad-435cc55b429c\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_label = 'OL7608 - Millenial Media'\n",
    "model_seed = '4sk|dwbo|xupk'\n",
    "\n",
    "pipeline_args = {\n",
    "    'qry':src_qry,\n",
    "    'seed':model_seed,\n",
    "    'bq_output_path':'kfpca_bq',\n",
    "    'ts':datetime.datetime.now().isoformat(),\n",
    "    'model_label':model_label\n",
    "}\n",
    "\n",
    "my_run = client.create_run_from_pipeline_func(pipeline_func=my_pipeline_func,\n",
    "                                     arguments=pipeline_args,\n",
    "                                     run_name=f\"{model_label}-{datetime.datetime.now().isoformat()}\",\n",
    "                                     experiment_name=\"KFP Demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://3fad3dc513a320a4-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/571d2171-a7e3-4214-b86a-b49e04d89483\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://3fad3dc513a320a4-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/fdac335c-3954-4e3a-9e6f-53b943c7395a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_label = 'OL8206 - Wedding Planning'\n",
    "model_seed = 'cjbq,fzdx,bzck'\n",
    "\n",
    "pipeline_args = {\n",
    "    'qry':src_qry,\n",
    "    'seed':model_seed,\n",
    "    'bq_output_path':'kfpca_bq',\n",
    "    'ts':datetime.datetime.now().isoformat(),\n",
    "    'model_label':model_label\n",
    "}\n",
    "\n",
    "my_run = client.create_run_from_pipeline_func(pipeline_func=my_pipeline_func,\n",
    "                                     arguments=pipeline_args,\n",
    "                                     run_name=f\"{model_label}-{datetime.datetime.now().isoformat()}\",\n",
    "                                     experiment_name=\"KFP Demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
